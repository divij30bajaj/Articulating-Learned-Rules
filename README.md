## Articulating-Learned-Rules

#### Background
This work is done as a submission to Owain Evans' MATS stream for the Winter Cohort 2024-25. The task is to evaluate if LLMs can learn and articulate simple classification rules. For this exercise, the preliminary experiemts were done using 6 LLMs - GPT-4, GPT-4o, Claude-3.5-Sonnet, Llama3-70B, Llama-3.1-70B and Gemini-1.5-Pro. All model APIs are initialized and queried in `models.py`. Replace `<API-KEY>` with your own API key for each LLM API service. 

A total of 6 classification tasks were chosen:
The input is labeled as ‘True’ if and only if
1. The input contains a time in a 12-hour format: Half of the input examples mentioned a random time in a 12-hour format (HH:MM AM/PM) and the other half mentioned a random time in a 24-hour format (HHMM hours). The query was randomly chosen to be from one of the two formats.
2. The input is enclosed in double dollar signs (\$$): Half of the input examples are enclosed in double dollar signs. For example, “\$$<Sentence>\$$”. The other half is kept unchanged and are simple and short English sentences. The query was randomly chosen to be from one of the two kinds.
3. The first word in the input is in all uppercase letters: Half of the input examples were simple and short English sentences, while the other half had the first word capitalized.
4. The input mentions apples: 50% of the input examples were sentences talking about apples, while the other half were similar sentences, but with “apples” replaced by another fruit from a pool of 5 fruits.
5. The input contains numbers spelled out in words: All input examples mention some number between 0 and 10. Half of these input examples mention the number in words, while the other half mentions in digits.
6. The input contains two sentences: Half of the input examples were simple and short English sentences, while the other half was a concatenation of two sentences of similar kinds.

#### Steps to Run
###### Generate Data
To run the code from scratch, the first step is to generate prompts for each task defined in `tasks.py`. To do so, run the following command with the desired values of arguments:

`python generate_data.py --num_examples_per_prompt=32 --num_prompts=100`

The above code uses some simple and short English sentences that were generated by ChatGPT. These sentences are stored in the `inputs` directory. The output of the above script will be stored in the `data` directory. 

###### Step 1: Evaluating Model Performance on Classification Tasks
Running `task_performance.py` (without any arguments) will print the accuracy of each model on each of the 6 tasks.

###### Step 2: Evaluating Articulation
`articulation_eval.py` runs the articulation evaluation on strong LLMs (GPT-4, GPT-4o and Claude-3.5-Sonnet). It compares the model responses with pre-defined sets of keywords that most likely mean that the articulation is correct. For further correction, the articulation and the predicted labels are written in (separate) files for manual analysis.

###### Step 3: Investigating Faithfulness
The reasoning saved from the previous step is used here. It is corrupted by either flipping the labels or replacing it with a series of periods. Then, the reasoning is introduced in the input instruction and the model is asked to answer given the reasoning. Results are written to separate files.


